# -*- coding: utf-8 -*-
"""2. 사이킷런

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ey1diWSv7_FOS6ePqmaM8bvLbTgHtAa5

## 2. 사이킷런

- 파이썬머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리
- 분류 , 회귀, 클러스터링, 차원축소
"""

import sklearn

"""## 2.1. 첫 번째 머신러닝 - 붓꽃 품종 예측

분류 : 지도학습은 학습을 위한 다양한 피처와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블 예측

- 학습데이터와 테스트데이터 세트로 지칭
"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split # 테스트 데이터와 학습 데이터 분리

# 붓꽃 데이터 세트로 로딩 -> 피처들과 데이터 값을 구성한 데이터 프레임
import pandas as pd

iris = load_iris()

iris_data = iris.data

iris_label = iris.target

# iris target 붓꽃 데이터 세트에서 레이블 데이터 numpy를 가짐
print('iris target 값:', iris_label)
print('iris target 명:', iris.target_names)

iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
iris_df['label'] = iris.target
iris_df.head(3)

X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)

dt_clf = DecisionTreeClassifier(random_state=11)
# 의사 결정 트리

# 학습 수행
dt_clf.fit(X_train, y_train)

# 학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 셋 수행
pred = dt_clf.predict(X_test)

from sklearn.metrics import accuracy_score
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))

"""- 데이터 세트 분리: 데이터를 학습 데이터와 테스트 데이터로 분리

- 모델 학습 : 학습 데이터를 기반으로 ML 알고리즘을 사용해 모델 학습

- 예측 수행 : 학습된 ML 모델을 이용해 테스트 데이터의 분류 예측

- 평가 : 예측된 결과값과 테스트 데이터의 실제 결과값을 비교하여 ML 성능 평가
"""



"""## 2.2 사이킷런 기반 프레임워크 익히기

- '.fit ' : ML 모델 학습
- 'predict' : 학습된 모델 예측


분류 알고리즘의 경우 classifier
회귀 알고리즘의 경우 Regressor

비지도헉숩: 차원축소, 클러스터링, 피처 추출
"""

from sklearn import datasets

# 붓꽃 데이터 로드
iris_data = datasets.load_iris()

# 데이터의 키 확인
keys = iris_data.keys()
print('붓꽃 데이터 세트의 키들:', keys)

# 객체의 키의 값 출력
print('\n feature_names 의 type:', type(iris_data.feature_names))
print(' feature_names 의 shape:', len(iris_data.feature_names))
print(iris_data.feature_names)

print('\n target_names 의 type:', type(iris_data.target_names))
print(' feature_names 의 shape:', len(iris_data.target_names))
print(iris_data.target_names)

"""## 2.3 Modl selection 모듈 소개


"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
dt_clf = DecisionTreeClassifier()
train_data = iris.data
test_data = iris.target
dt_clf.fit(train_data, test_data)

pred = dt_clf.predict(train_data)
print('예측 정확도:', accuracy_score(test_data, pred))

"""예측 결과가 100%인 이유는 이미 학습한 학습 데이터 세트를 기반으로 예측

'train_test_split()'을 이용하여 테스트 데이터를 0.3, 학습 데이터를 0.7로 분리-> random_state= 121로 설정
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

dt_clf = DecisionTreeClassifier()
iris_data = load_iris()

X_train,X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,
                                                   test_size = 0.3, random_state =121)

dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))

"""# 교차검증

과적합은 모델이 학습 데이터에만 과도하게 최적화되어 실제 예측을 했을 경우 정확도가 과도하게 떨어지는 경우 말함.

이러한 문제점을 개선하기 위해 교차 검증을 시행함
별도의 여러세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행함

k 폴드 교차 검증 : k 개의 데이터 폴드 세트를 만들어서 k번 만큼 각 폴드 세트에 학습과 검증평가를 반복적으로 수행하는 방법


"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import numpy as np
from sklearn.datasets import load_iris

# 데이터 로드
iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state=156)

# 5개의 폴드 세트로 분리하는 KFold 객체 생성
kfold = KFold(n_splits=5)
cv_accuracy = []
print('붓꽃 데이터 세트 크기:', features.shape[0])

# 초기화
n_iter = 0

# kfold 객체의 split()를 호출하면 폴드별 학습용,검증용 테스트의 로우 인덱스로 array로 변환

for train_index, test_index in kfold.split(features):
    # kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]

    # 학습 및 예측
    dt_clf.fit(X_train, y_train)
    pred = dt_clf.predict(X_test)
    n_iter += 1

    # 반복 시마다 정확도 측정
    accuracy = np.round(accuracy_score(y_test, pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter, test_index))
    cv_accuracy.append(accuracy)

# 개별 iteration별 정확도를 합하여 평균 정확도 계산
print('\n## 평균 검증 정확도:', np.mean(cv_accuracy))

# stratified k폴드 : 불균형한 분포도를 가진 데이터를 데이터 집합을 위한 k폴드 방식
import pandas as pd

iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['label']=iris.target
iris_df['label'].value_counts()

kfold = KFold(n_splits=3)
n_iter = 0

for train_index, test_index in kfold.split(iris_df):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]
    print('## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())

from sklearn.model_selection import StratifiedKFold

# StratifiedKFold 객체 생성
skf = StratifiedKFold(n_splits=3)
n_iter = 0

for train_index, test_index in skf.split(iris_df, iris_df['label']):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]
    print('## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())
    print('검증 레이블 데이터 분포:\n', label_test.value_counts())

dt_clf = DecisionTreeClassifier(random_state=156)

skfold = StratifiedKFold(n_splits=3)
n_iter=0
cv_accuracy=[]

# StratifiedKFold의 split() 호출시 반드시 레이블 데이터 세트도 추가 입력 필요
for train_index, test_index in skfold.split(features, label):
    # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    #학습 및 예측
    dt_clf.fit(X_train , y_train)
    pred = dt_clf.predict(X_test)

    # 반복 시 마다 정확도 측정
    n_iter += 1
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))

# 교차검증을보다 간편하게 - cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.datasets import load_iris

iris_data = load_iris()
dt_clf = DecisionTreeClassifier(random_state=156)

data = iris_data.data
label = iris_data.target

scores = cross_val_score(dt_clf, data, label, scoring='accuracy', cv=3)
print('교차 검증별 정확도:', np.round(scores, 4))
print('평균 검증 정확도:', np.round(np.mean(scores), 4))

# GridSearchCV - 교차검증과 최적 하이퍼 파라미터 튜닝을 한번에
grid_parameters = {'max_depth': [1, 2, 3],
                   'min_sampled_split': [2, 3]
                   }

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# 데이터를 로딩하고 학습 데이터와 테스트 데이터 분리
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,
                                                    test_size=0.2, random_state=12)
dtree = DecisionTreeClassifier()

### 파라미터를 딕셔너리 형태로 설정
parameters = {'max_depth': [1, 2, 3], 'min_samples_split': [2, 3]}

import pandas as pd

# param_grid 하이퍼 파라미터를 3개의 train, test set fold로 나누어 테스트 수행 설정
### refit=True가 default임. True이면 가장 좋은 파라미터 설정으로 재학습시킴
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)

# 붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습/평가
grid_dtree.fit(X_train, y_train)

# GridSearchCV 결과를 추출해 DataFrame으로 변환
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score',
           'split0_test_score', 'split1_test_score', 'split2_test_score']]

print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_))

# GridSearchCV의 refit으로 이미 학습된 estimator 반환
estimator = grid_dtree.best_estimator_

pred = estimator.predict(X_test)
print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))

"""GridSearchCV

## 2.3 데이터 전처리
"""

# 레이블 인코딩 : 카테고리 피처를 코드형 숫자 값으로 반환
from sklearn.preprocessing import LabelEncoder

items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']

# LabelEncoder를 객체로 생성한 뒤, fit과 transform()으로 레이블 인코딩 수행
encoder = LabelEncoder()
encoder.fit(items)
labels = encoder.transform(items)
labels

encoder.classes_
# 속성값을 알 수 있음

# 디코딩 원본 값
encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3])

# 원-핫 인코딩 : 피처값의 유형에 따라 새로운 피처를 추가해 고유값에 해당하는 칼럼에만 1을 표시하고
# 나머지ㅣ 칼럼에는 0을 표시
from sklearn.preprocessing import OneHotEncoder
import numpy as np

items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']

# 2차원 ndarray로 변환
items = np.array(items).reshape(-1, 1)

# 원 핫 인코딩을 적용
oh_encoder = OneHotEncoder()
oh_encoder.fit(items)
oh_labels = oh_encoder.transform(items)

print('원-핫 인코딩 데이터')
print(oh_labels.toarray())
print('원-핫 인코딩 데이터 차원')
print(oh_labels.shape)

# get_dummies
import pandas as pd

df = pd.DataFrame({'item': ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']})
encoded_df = pd.get_dummies(df, dtype=int)
encoded_df

"""피처스케일링과 정규화

- 피처 스케일링 : 서로 다른 변수 값 범위를 일정한 수준으로 맞추는 작업-> 표준화 / 정규화
"""

# StandardScaler
# 개별 피처를 평균이 0이고 분산이 1인 값으로 변환

from sklearn.datasets import load_iris
import pandas as pd

# 붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환
iris = load_iris()  # 함수 호출에 () 추가
iris_data = iris.data
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)

print(iris_df.head())

print('feature 들의 평균 값')
print(iris_df.mean())
print('\nfeature 들의 분산 값')
print(iris_df.var())

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
print('feature 들의 평균 값')
print(iris_df_scaled.mean())

# MinMaxScaler
# 데이터 값을 0과 1사이의 범위값으로 변환
# 음수 값이 있으면 -1에서 1값으로 변환

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
print('feature들의 최솟값')
print(iris_df_scaled.min())
print('\nfeature들의 최댓값')
print(iris_df_scaled.max())

